{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "syn-oea-sekou"
		},
		"syn-oea-sekou-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'syn-oea-sekou-WorkspaceDefaultSqlServer'"
		},
		"syn-oea-sekou-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://stoeasekou.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/syn-oea-sekou-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('syn-oea-sekou-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/syn-oea-sekou-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('syn-oea-sekou-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/1_read_me')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "3c7ba245-972f-4174-b1a2-14fe831e87ce"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"![OEA](https://openeducationanalytics.org/assets/imgs/img_oea_logo.png)\n",
							"# OEA and the OEA Framework\n",
							"\n",
							"[OEA](https://openeducationanalytics.org/) is the overarching community and ecosystem centered around the effective and responsible use of data and analytics in education.\n",
							"\n",
							"The [OEA framework](https://github.com/microsoft/OpenEduAnalytics/tree/main/framework) is an open source python library and synapse pipeline assets - built in collaboration with the OEA community - that simplifies the process of working with the data in your data lake in a way that follows a standardized data lake architecture and data processing best practices through use of [Apache Spark](https://spark.apache.org/) and [delta lake](https://delta.io/) technologies.\n",
							"\n",
							"Listed below are 3 included examples that demonstrate the usage of the OEA framework."
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"# Example #1: End to end (collect, prep, view)\n",
							"The OEA framework comes with a set of Synapse pipelines that demonstrate how to extract data from data sources with common interfaces.\n",
							"\n",
							"By clicking on \"Integrate\" in the left nav bar and opening \"example_main_pipeline\" you can run an example pipeline that does the following:\n",
							"- 1. Retrieves data from an http endpoint\n",
							"- 2. Lands the data in the stage1np directory\n",
							"- 3. Ingests the data by first running a pseudonymization process, then writing pseudonymized data to delta tables in stage2p and writing non-pseudonymized data to delta tables in stage2np\n",
							"- 4. Creates a spark db that points to the delta tables in stage2p and stage2np\n",
							"- 5. Creates a sql serverless db with views pointing to the delta tables in stage2p and stage2np\n",
							"\n",
							"You can then run the pipeline in the Reset folder called \"reset_all_for_source\" to reset everything in the data lake that was done in the \"example_main_pipeline\"."
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Example #2: batch data processing\n",
							"The notebook **_2_batch_processing_demo_** provides a self-contained demonstration of landing and ingesting 3 different types of batch data sets:\n",
							"\n",
							"1. [Incremental data](https://github.com/microsoft/OpenEduAnalytics/tree/main/framework#1-incremental-data)\n",
							"2. [Delta data](https://github.com/microsoft/OpenEduAnalytics/tree/main/framework#2-delta-data-change-data)\n",
							"3. [Snapshot data](https://github.com/microsoft/OpenEduAnalytics/tree/main/framework#3-snapshot-data)\n",
							"\n",
							"Open that notebook and walk through each cell for the details."
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Example #3: data generation demo\n",
							"When learning to work synapse studio and the OEA framework, and when developing the data exploration and data prep scripts you need, it's especially helpful to have test data sets to work with.\n",
							"\n",
							"The notebook **_3_data_generation_demo_** data generation demo shows how to generate test data sets across multiple fictional schools for testing purposes."
						],
						"attachments": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/2_batch_processing_demo')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "50d33abc-63ae-4728-a039-a46d65dae384"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# OEA Demo\r\n",
							"This notebook demonstrates the batch processing features of the OEA framework."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"%run /OEA_py"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Incremental batches"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# reset this example (deletes data in stage1np/example, stage2np/example, and stage2p/example)\r\n",
							"oea.delete_data_source('example')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Land the first batch of test data\r\n",
							"df1 = spark.createDataFrame([(1,'Joe','English','2021'), (2,'Helen','English','2021')], ['id', 'name', 'language', 'school_year'])\r\n",
							"oea.land('example', 'student', df1)\r\n",
							"# show what's landed in stage1\r\n",
							"df = oea.load_csv('stage1np/example/student')\r\n",
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# ingest the first batch of test data into stage2\r\n",
							"example_schema = [['id', 'string', 'hash'], ['name', 'string', 'mask'], ['language', 'string', 'no-op'], ['school_year', 'string', 'partition-by']]\r\n",
							"oea.ingest_incremental_data('example', 'student', example_schema, 'school_year', 'id')\r\n",
							"\r\n",
							"# show what's in stage2\r\n",
							"df = oea.load_delta('stage2np/example/student_lookup')\r\n",
							"df.show()\r\n",
							"df = oea.load_delta('stage2p/example/student_pseudo')\r\n",
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# land the second batch of test data\r\n",
							"df2 = spark.createDataFrame([(3,'Elisa','Spanish','2021'), (4,'Lily','English','2021')], ['id', 'name', 'language', 'school_year'])\r\n",
							"oea.land('example', 'student', df2)\r\n",
							"# show the comprehensive set of data landed in stage1\r\n",
							"df = oea.load_csv('stage1np/example/student')\r\n",
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# ingest the second batch of test data into stage2\r\n",
							"oea.ingest_incremental_data('example', 'student', example_schema, 'school_year', 'id')\r\n",
							"\r\n",
							"# show what's in stage2\r\n",
							"df = oea.load_delta('stage2np/example/student_lookup')\r\n",
							"df.show()\r\n",
							"df = oea.load_delta('stage2p/example/student_pseudo')\r\n",
							"df.show()\r\n",
							"df.printSchema()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Delta batches"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# reset this example (deletes data in stage1np/example, stage2np/example, and stage2p/example)\r\n",
							"oea.delete_data_source('delta_example')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Land the first batch of test data\r\n",
							"df1 = spark.createDataFrame([(1,'Joseph','English','2021'), (2,'Helen','English','2021')], ['id', 'name', 'language', 'school_year'])\r\n",
							"oea.land('delta_example', 'student', df1)\r\n",
							"\r\n",
							"# show what's landed in stage1\r\n",
							"df = oea.load_csv('stage1np/delta_example/student')\r\n",
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# ingest the first batch of test data into stage2\r\n",
							"example_schema = [['id', 'string', 'hash'], ['name', 'string', 'mask'], ['language', 'string', 'no-op'], ['school_year', 'string', 'partition-by']]\r\n",
							"oea.ingest_delta_data('delta_example', 'student', example_schema, 'school_year')\r\n",
							"\r\n",
							"# show what's in stage2\r\n",
							"df = oea.load_delta('stage2np/delta_example/student_lookup')\r\n",
							"df.show()\r\n",
							"df = oea.load_delta('stage2p/delta_example/student_pseudo')\r\n",
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Land the second batch of test data\r\n",
							"df2 = spark.createDataFrame([(1,'Joseph','Spanish','2021'), (3,'Elisa','Spanish','2021')], ['id', 'name', 'language', 'school_year'])\r\n",
							"oea.land('delta_example', 'student', df2)\r\n",
							"\r\n",
							"# show what's landed in stage1\r\n",
							"df = oea.load_csv('stage1np/delta_example/student')\r\n",
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# ingest the second batch of test data into stage2\r\n",
							"oea.ingest_delta_data('delta_example', 'student', example_schema, 'school_year')\r\n",
							"\r\n",
							"# show what's in stage2\r\n",
							"df = oea.load_delta('stage2np/delta_example/student_lookup')\r\n",
							"df.show()\r\n",
							"df = oea.load_delta('stage2p/delta_example/student_pseudo')\r\n",
							"df.show()\r\n",
							"df.printSchema()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Snapshot batches"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# reset this example (deletes data in stage1np/example, stage2np/example, and stage2p/example)\r\n",
							"oea.delete_data_source('snapshot_example')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# land data in stage1\r\n",
							"df1 = spark.createDataFrame([(1,'Joseph','English','2021'), (2,'Helen','English','2021')], ['id', 'name', 'language', 'school_year'])\r\n",
							"oea.land('snapshot_example', 'student', df1)\r\n",
							"\r\n",
							"# show what's landed in stage1\r\n",
							"df = oea.load_csv('stage1np/snapshot_example/student')\r\n",
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# process data from stage1 into stage2\r\n",
							"example_schema = [['id', 'string', 'hash'], ['name', 'string', 'mask'], ['language', 'string', 'no-op'], ['school_year', 'string', 'partition-by']]\r\n",
							"oea.ingest_snapshot_data('snapshot_example', 'student', example_schema, 'school_year')\r\n",
							"\r\n",
							"# show what's in stage2\r\n",
							"df = oea.load_delta('stage2np/snapshot_example/student_lookup')\r\n",
							"df.show()\r\n",
							"df = oea.load_delta('stage2p/snapshot_example/student_pseudo')\r\n",
							"df.show()\r\n",
							"df.printSchema()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# land the second test data batch in stage1\r\n",
							"df2 = spark.createDataFrame([(1,'Joseph','Spanish','2021'), (3,'Elisa','Spanish','2021')], ['id', 'name', 'language', 'school_year'])\r\n",
							"oea.land('snapshot_example', 'student', df2)\r\n",
							"\r\n",
							"# show what's landed in stage1\r\n",
							"df = oea.load_csv('stage1np/snapshot_example/student')\r\n",
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# process data from stage1 into stage2\r\n",
							"example_schema = [['id', 'string', 'hash'], ['name', 'string', 'mask'], ['language', 'string', 'no-op'], ['school_year', 'string', 'partition-by']]\r\n",
							"oea.ingest_snapshot_data('snapshot_example', 'student', example_schema, 'school_year')\r\n",
							"\r\n",
							"# show what's in stage2\r\n",
							"df = oea.load_delta('stage2np/snapshot_example/student_lookup')\r\n",
							"df.show()\r\n",
							"df = oea.load_delta('stage2p/snapshot_example/student_pseudo')\r\n",
							"df.show()\r\n",
							"df.printSchema()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/3_data_generation_demo')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "9dc5a72c-a5db-45f4-9e6d-8c68c81ac0f2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Data Generation Example\r\n",
							"This notebook demonstrates how to use the EdFiDataGenerator to generate test student data in the Ed-Fi format for as many schools as specified.\r\n",
							"\r\n",
							"To generate test Ed-Fi data, simple run this notebook.\r\n",
							"The test data will be generated in json format and written to stage1np/test_data in your data lake."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run OEA_py"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run DataGen_py"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"dg = EdFiDataGenerator()\r\n",
							"writer = DataLakeWriter(oea.stage1np + '/test_data')\r\n",
							"dg.generate_data(2, writer)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/spark3p1sm')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 5,
					"minNodeCount": 3
				},
				"nodeCount": 3,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.1",
				"libraryRequirements": {
					"content": "opencensus-ext-azure >= 1.0.8\r\nFaker >= 8.0",
					"filename": "/usr/csuser/clouddrive/OpenEduAnalytics/framework/requirements.txt",
					"time": "2022-06-26T11:36:00.1649594Z"
				},
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus"
		}
	]
}